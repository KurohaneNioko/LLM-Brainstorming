{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d79f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, traceback, argparse\n",
    "# import openai\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import transformers\n",
    "from multiprocessing import Process, Pool\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from fastparquet import ParquetFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1490e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# glm_device = \"cuda:0\"\n",
    "# glm_tokenizer = transformers.AutoTokenizer.from_pretrained(\"GPTs/chatglm3-6b\", trust_remote_code=True)\n",
    "# glm_model = transformers.AutoModel.from_pretrained(\"GPTs/chatglm3-6b\", device_map=\"cuda:0\", trust_remote_code=True).half()\n",
    "# glm_model = glm_model.eval()\n",
    "# # response, history = glm_model.chat(glm_tokenizer, \"你好\", history=[])\n",
    "# def chat_glm3(glm_model, glm_tokenizer, glm_device, prompt, history=[]):\n",
    "#     response, history = glm_model.chat(glm_tokenizer, prompt, history=history)\n",
    "#     # print(history)\n",
    "#     return response, history\n",
    "# model_a_name = \"chatglm3-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99771da-9e99-490a-b1ac-84471fea8b7e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# yi_device = \"cuda:0\"\n",
    "# yi_tokenizer = transformers.AutoTokenizer.from_pretrained('GPTs/Yi-6B', use_fast=False)\n",
    "# yi_model = transformers.AutoModelForCausalLM.from_pretrained('GPTs/Yi-6B', device_map=yi_device, torch_dtype='auto').eval()\n",
    "# def chat_yi(yi_model, yi_tokenizer, yi_device, prompt, messages=None):\n",
    "#     if messages == None:\n",
    "#         messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     else:\n",
    "#         messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "#     input_ids = yi_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "#     output_ids = yi_model.generate(input_ids.to('cuda'))\n",
    "#     response = yi_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "#     messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "#     return response, messages\n",
    "# model_a_name = \"yi-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40909d2-c318-4f95-b064-138620dd3a22",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# gemma_device = \"cuda:0\"\n",
    "# gemma_tokenizer = transformers.AutoTokenizer.from_pretrained('GPTs/gemma-7b')\n",
    "# gemma_model = transformers.AutoModelForCausalLM.from_pretrained('GPTs/gemma-7b', device_map=gemma_device, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\").eval()\n",
    "# def chat_gemma(gemma_model, gemma_tokenizer, gemma_device, prompt, messages=None):\n",
    "#     if messages == None:\n",
    "#         messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     else:\n",
    "#         messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "#     outputs = gemma_model.generate(input_ids=gemma_tokenizer.encode(\n",
    "#         gemma_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True), \n",
    "#         add_special_tokens=False, return_tensors=\"pt\").to(gemma_model.device), max_new_tokens=150)\n",
    "#     response = gemma_tokenizer.decode(outputs[0])\n",
    "#     messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "#     return response, messages\n",
    "# model_a_name = \"gemma-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0a9e0-3cda-471d-baca-76b5116f8e4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# phi_device = \"cuda:0\"\n",
    "# phi_model = transformers.AutoModelForCausalLM.from_pretrained(\"GPTs/phi-1\", torch_dtype=\"auto\", trust_remote_code=True, device_map=phi_device)\n",
    "# phi_tokenizer = transformers.AutoTokenizer.from_pretrained(\"GPTs/phi-1\", trust_remote_code=True)\n",
    "# model_a_name = \"phi-1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea56c0-ed05-4aa0-8adf-90a41d21d3b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tiny_device = \"cuda:0\"\n",
    "# tiny_model = transformers.AutoModelForCausalLM.from_pretrained('GPTs/TinyLlama-1.1B', device_map=tiny_device, torch_dtype='auto').eval()\n",
    "# tiny_tokenizer = transformers.AutoTokenizer.from_pretrained('GPTs/TinyLlama-1.1B', truncation_side='left')\n",
    "# model_b_name = 'TinyLlama-1.1B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd410a-01c3-4017-accf-dc743ce8421a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# baichuan_device = \"cuda:0\"\n",
    "# baichuan_tokenizer = transformers.AutoTokenizer.from_pretrained(os.path.expanduser(\"~/GPTs/Baichuan2-7B\"), trust_remote_code=True)\n",
    "# baichuan_model = transformers.AutoModelForCausalLM.from_pretrained(os.path.expanduser(\"~/GPTs/Baichuan2-7B\"), device_map=baichuan_device, torch_dtype=torch.bfloat16, trust_remote_code=True).half()\n",
    "# baichuan_model.generation_config = transformers.GenerationConfig.from_pretrained(os.path.expanduser(\"~/GPTs/Baichuan2-7B\"))\n",
    "# baichuan_model = baichuan_model.eval()\n",
    "# baichuan_name = \"Baichuan2-7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adfd6b4-3c11-4483-a161-fabe2a67852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qw_device = \"cuda:0\"\n",
    "# qw_model = transformers.AutoModelForCausalLM.from_pretrained(os.path.expanduser(\"~/GPTs/qwen1.5-7b\"), torch_dtype=\"auto\", device_map=qw_device, attn_implementation=\"flash_attention_2\").eval()\n",
    "# qw_tokenizer = transformers.AutoTokenizer.from_pretrained(os.path.expanduser(\"~/GPTs/qwen1.5-7b\"))\n",
    "# # Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n",
    "# # model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-14B-Chat\", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n",
    "# \"\"\"\n",
    "# def chat_qwen(qw_model, qw_tokenizer, qw_device, prompt, messages=None):\n",
    "#     if messages is None:\n",
    "#         messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "#     messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "#     new_input = qw_tokenizer([qw_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)], return_tensors=\"pt\").to(qw_device)\n",
    "#     resp = qw_tokenizer.batch_decode([output_ids[len(input_ids):] for input_ids, output_ids in zip(new_input.input_ids, qw_model.generate(new_input.input_ids, max_new_tokens=512, pad_token_id=qw_tokenizer.eos_token_id))], \n",
    "#                                  skip_special_tokens=True)[0]\n",
    "#     messages.append({\"role\": \"assistant\", \"content\": resp})\n",
    "#     return resp, messages\n",
    "# \"\"\"\n",
    "# qw_name = \"qwen1.5-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fbe54-8bcc-4e84-ac16-43f57f1d0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "qw_device = \"cuda:0\"\n",
    "qw_model = transformers.AutoModelForCausalLM.from_pretrained(os.path.expanduser(\"~/GPTs/Qwen2-7B-Instruct\"), torch_dtype=\"auto\", device_map=qw_device, attn_implementation=\"flash_attention_2\")\n",
    "qw_tokenizer = transformers.AutoTokenizer.from_pretrained(os.path.expanduser(\"~/GPTs/Qwen2-7B-Instruct\"))\n",
    "# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n",
    "# qw_model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-14B-Chat\", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n",
    "qw_name = \"qwen2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91e2b3-3497-4d82-9081-e0c90afd936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_device = \"cuda:1\"\n",
    "llama_model = transformers.AutoModelForCausalLM.from_pretrained(os.path.expanduser('~/GPTs/llama3.1-8B-Instruct'), device_map=llama_device, torch_dtype='auto', attn_implementation=\"flash_attention_2\").eval()\n",
    "llama_tokenizer = transformers.AutoTokenizer.from_pretrained(os.path.expanduser('~/GPTs/llama3.1-8B-Instruct'))#, truncation_side='left')\n",
    "llama_name = 'llama3.1-8b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f98482-ab90-49e1-b92c-16eaea5b7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral_device = \"cpu\"\n",
    "mistral_device = \"cuda:0\"\n",
    "mistral_model = transformers.AutoModelForCausalLM.from_pretrained(os.path.expanduser(\"~/GPTs/Mistral-7B-Instruct-v0.3\"), torch_dtype=\"auto\", device_map=mistral_device,\n",
    "                                                                 attn_implementation=\"flash_attention_2\"\n",
    "                                                                 )\n",
    "mistral_tokenizer = transformers.AutoTokenizer.from_pretrained(os.path.expanduser(\"~/GPTs/Mistral-7B-Instruct-v0.3\"))\n",
    "mistral_name = \"Mistral-7B-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724805d-a605-4d7b-a9a3-32f6b62d7548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral_device = \"cpu\"\n",
    "# mistral_model = transformers.AutoModelForCausalLM.from_pretrained(os.path.expanduser(\"~/GPTs/Mistral-v0.2-7B\"), torch_dtype=\"auto\", device_map=mistral_device, attn_implementation=\"flash_attention_2\").eval()\n",
    "# mistral_tokenizer = transformers.AutoTokenizer.from_pretrained(os.path.expanduser(\"~/GPTs/Mistral-v0.2-7B\"))\n",
    "# mistral_name = \"Mistral-v0.2-7B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b420b8e-8161-41c0-8f63-0133c09bcf31",
   "metadata": {},
   "source": [
    "## Brainstorming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398fb1f-937f-4afa-a090-66cdf65067a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def get_result_df(dataset_name, models_info, timestamp, max_round, save_last_n_round, result_dir):\n",
    "    # model_num = len(models_info)\n",
    "    model_name_list = list(map(lambda x: x['name'], models_info))\n",
    "    result_csv_filename = f\"{dataset_name}_{'_'.join(model_name_list)}_{timestamp}_{max_round}_Last{save_last_n_round}Round.csv\"\n",
    "    csv_path = os.path.join(result_dir, result_csv_filename)\n",
    "    if os.path.exists(csv_path):\n",
    "        result_df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        result_df = pd.DataFrame(columns=(['ith','label']+[f\"{n}_first_ans\" for n in model_name_list]+['game_ans','rounds']))\n",
    "        result_df.to_csv(csv_path, index=False)\n",
    "    return result_df\n",
    "\n",
    "def to_result_df(dataset_name, models_info, \n",
    "                 timestamp, max_round, save_last_n_round, result_dir, # runtime\n",
    "                 data_row, # to result_csv\n",
    "                 ):\n",
    "    model_name_list = list(map(lambda x: x['name'], models_info))\n",
    "    result_csv_filename = f\"{dataset_name}_{'_'.join(model_name_list)}_{timestamp}_{max_round}_Last{save_last_n_round}Round.csv\"\n",
    "    csv_path = os.path.join(result_dir, result_csv_filename)\n",
    "    if os.path.exists(csv_path):\n",
    "        result_df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        result_df = pd.DataFrame(columns=(['ith','label']+[f\"{n}_first_ans\" for n in model_name_list]+['game_ans','rounds']))\n",
    "        result_df.to_csv(csv_path, index=False)\n",
    "    result_df.loc[len(result_df)] = data_row\n",
    "    # result_df = pd.concat([result_df, pd.DataFrame([{\n",
    "    #         'ith':ith, 'label':label, 'a_first_ans':a_first_ans,'b_first_ans':b_first_ans,\n",
    "    #         'game_ans':game_ans,'rounds':rounds}])],ignore_index=True)\n",
    "    result_df.to_csv(csv_path, index=False)\n",
    "    return result_df\n",
    "\n",
    "def save_dialogs(dialog_items, converged, split_mark, dataset_name, idx, models_info, timestamp, result_dir):\n",
    "    dialog_dir = os.path.join(result_dir, f\"dialog_{timestamp}_LastOnly\")\n",
    "    model_name_list = list(map(lambda x: x['name'], models_info))\n",
    "    if not os.path.exists(dialog_dir): os.mkdir(dialog_dir)\n",
    "    dialog_filename = f\"{dataset_name}_{'_'.join(model_name_list)}_{timestamp}_{idx}_{'Y' if converged else 'N'}.txt\"\n",
    "    dialog_fp = open(os.path.join(dialog_dir, dialog_filename), 'w+', encoding=\"utf-8\")\n",
    "    for resp in dialog_items:\n",
    "        dialog_fp.write(resp+'\\n')\n",
    "        dialog_fp.write(split_mark)\n",
    "    dialog_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabc398-534c-4319-b587-8546d7a0e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brainstorm(models_info, max_round, save_last_n_round, timestamp, result_dir, # runtime\n",
    "               dataset_df, dataset_name, equal_func, init_prompt_template, game_prompt_template, parse_ans, # data & prompt & utils\n",
    "               # parallel_questions: int, # run *2 chat process\n",
    "               # start_offset=0 # another 2 GPUs node!\n",
    "              ):\n",
    "    dialog_item_split = \"**/-|+|=\\**\\n\"\n",
    "    model_num = len(models_info)\n",
    "    \n",
    "    # pool = Pool(processes=parallel_questions*2, maxtasksperchild=1)\n",
    "    # proc_pointer = [[None, None] for _ in range(parallel_questions)]\n",
    "    # runtime_record = [reset_runtime() for _ in range(parallel_questions)]\n",
    "    result_df = get_result_df(dataset_name, models_info, timestamp, max_round, save_last_n_round, result_dir)\n",
    "    # pbar = tqdm(range(len(dataset_df))) #total=len(dataset_df)-start_offset)\n",
    "    restart = True\n",
    "    # question_index = start_offset\n",
    "    # finished_count = 0\n",
    "    for idx in (pbar:=tqdm(range(len(dataset_df)))):\n",
    "        if idx in result_df['ith'].values:\n",
    "            # pbar.update(1)\n",
    "            continue\n",
    "        if len(result_df)>0:\n",
    "            if restart:\n",
    "                label_so_far = result_df.label.to_list()\n",
    "                model_1st_column_names = result_df.columns[2:-2]\n",
    "                models_so_far = [getattr(result_df, col_name).to_list() for col_name in model_1st_column_names]\n",
    "                game_so_far = result_df.game_ans.to_list()\n",
    "                model_1st_corrects = [list(map(lambda x: equal_func(x[0],x[1]), zip(label_so_far, so_far))).count(True) for so_far in models_so_far]\n",
    "                # a_correct = list(map(lambda x: equal_func(x[0],x[1]), zip(label_so_far, model_a_so_far))).count(True)\n",
    "                # b_correct = list(map(lambda x: equal_func(x[0],x[1]), zip(label_so_far, model_b_so_far))).count(True)\n",
    "                game_correct = list(map(lambda x: equal_func(x[0],x[1]), zip(label_so_far, game_so_far))).count(True)\n",
    "                # consensus_wrong = list(map(lambda x: (not equal_func(x[0],x[1])) and (x[1] != NOT_CONVERGE_MARK), \n",
    "                #                            zip(label_so_far, game_so_far))).count(True)\n",
    "                restart = False\n",
    "        else:\n",
    "            model_1st_corrects = [0] * model_num\n",
    "            game_correct = 0\n",
    "        # idx-th problem, game rounds\n",
    "        answers_for_majority = []\n",
    "        # refractor dataframe before this calling\n",
    "        # choice: <Q-stem><choices> | <choice>\n",
    "        # gsm like: <Question> | <numerical number>\n",
    "        question = dataset_df.iloc[idx].question\n",
    "        subject = dataset_df.iloc[idx].subject\n",
    "        answer = dataset_df.iloc[idx].answer\n",
    "        question = init_prompt_template(dataset_df.iloc[idx].question, subject)\n",
    "        dialog_item = [question]\n",
    "        for round in range(max_round):\n",
    "            torch.cuda.empty_cache()\n",
    "            pool = Pool(processes=model_num, maxtasksperchild=1)\n",
    "            if round == 0:\n",
    "                results = [pool.apply_async(d['chatcaller'], (d['obj'], d['tokenizer'], d['device'], question,) ) for d in models_info]#[:-1]]\n",
    "                # # 3rd model in main thread\n",
    "                # d = models_info[-1]\n",
    "                # results_last = d['chatcaller'](d['obj'], d['tokenizer'], d['device'], question,)\n",
    "            else:\n",
    "                if round > save_last_n_round:    # shave history\n",
    "                    histories = [(history[:(3 if history[0]['role'] == \"system\" else 2)] + history[-2*save_last_n_round:])\n",
    "                                 for _, history in resp_history]\n",
    "                else:\n",
    "                    histories = [history for _, history in resp_history]\n",
    "                # get responses of all models from last round\n",
    "                resps = []\n",
    "                for i, (resp, _) in enumerate(resp_history):\n",
    "                    resps.append(resp)\n",
    "                # make game prompts\n",
    "                game_prompts = []\n",
    "                for i in range(model_num):\n",
    "                    resps_game = resps[:].pop(i)\n",
    "                    game_prompts.append(game_prompt_template(resps_game))\n",
    "                results = [pool.apply_async(d['chatcaller'], (d['obj'], d['tokenizer'], d['device'], game_prompts[i], histories[i]) ) for i,d in enumerate(models_info)]\n",
    "                # pool.close()\n",
    "                # pool.join()\n",
    "                # resp_history = [pointer.get() for pointer in results]\n",
    "                # # 3rd model in main thread\n",
    "                # d = models_info[-1]\n",
    "                # results_last = d['chatcaller'](d['obj'], d['tokenizer'], d['device'], game_prompts[-1], histories[-1])\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            resp_history = [pointer.get() for pointer in results]\n",
    "            # # 3rd model in main thread\n",
    "            # resp_history.append(results_last)\n",
    "            parsed_answers = []\n",
    "            # save dialog\n",
    "            for i,(r,_) in enumerate(resp_history):\n",
    "                dialog_item.append(f\"{models_info[i]['name']}: {r}\")\n",
    "                p_r = parse_ans(r)\n",
    "                parsed_answers.append(p_r)\n",
    "                answers_for_majority.append(p_r)\n",
    "            # for first round\n",
    "            if round == 0: \n",
    "                first_answers = parsed_answers[:]\n",
    "                for i,a in enumerate(parsed_answers[:]):\n",
    "                    if equal_func(a, answer):\n",
    "                       model_1st_corrects[i] += 1\n",
    "            # set `converged`\n",
    "            converged = True\n",
    "            for i in range(len(parsed_answers)-1):\n",
    "                converged = converged & (equal_func(parsed_answers[i], parsed_answers[i+1]))\n",
    "                if not converged: break\n",
    "            # finish\n",
    "            if converged or round == max_round -1:\n",
    "                if converged:\n",
    "                    if equal_func(parsed_answers[0], answer): game_correct += 1\n",
    "                    # else: consensus_wrong += 1\n",
    "                    game_ans = parsed_answers[0]\n",
    "                else: # not converged, reach max_round. majority vote when no converge\n",
    "                    try:\n",
    "                        v = Counter(answers_for_majority)\n",
    "                        game_ans = v.most_common(1)[0][0]\n",
    "                    except:\n",
    "                        choices = [\"A\",\"B\",\"C\",\"D\"]\n",
    "                        if \"or (E)\" in question: choices.append(\"E\")\n",
    "                        game_ans = np.random.choice(choices)\n",
    "                    if equal_func(game_ans, answer): game_correct += 1\n",
    "                result_df = to_result_df(dataset_name, models_info, timestamp, max_round, save_last_n_round, result_dir,\n",
    "                                         data_row=[idx, answer]+first_answers+[game_ans, round])\n",
    "                save_dialogs(dialog_item, converged, dialog_item_split, dataset_name,  idx, models_info, timestamp, result_dir)\n",
    "                del resp_history\n",
    "                break\n",
    "        each_model_str = \", \".join([f\"{d['name']}={model_1st_corrects[i]/(idx+1):.3f}\" for i,d in enumerate(models_info)])\n",
    "        pbar.set_description(f'{dataset_name} Game={game_correct/(idx+1):.3f}, {each_model_str}')\n",
    "        # {model_a_name}={a_correct/(idx+1):.3f}, {model_b_name}={b_correct/(idx+1):.3f}\")\n",
    "    # print(f\"Subject: {subject} Acc: Game={game_correct/(idx+1):.3f}, {model_a_name}={a_correct/(idx+1):.3f}, {model_b_name}={b_correct/(idx+1):.3f}\")\n",
    "    label_so_far = result_df.label.to_list()\n",
    "    model_1st_column_names = result_df.columns[2:-2]\n",
    "    models_so_far = [getattr(result_df, col_name).to_list() for col_name in model_1st_column_names]\n",
    "    game_so_far = result_df.game_ans.to_list()\n",
    "    model_1st_corrects = [list(map(lambda x: equal_func(x[0],x[1]), zip(label_so_far, so_far))).count(True) for so_far in models_so_far]\n",
    "    game_correct = list(map(lambda x: equal_func(x[0],x[1]), zip(label_so_far, game_so_far))).count(True)\n",
    "    each_model_str = \", \".join([f\"{d['name']}={model_1st_corrects[i]/(len(result_df)):.3f}\" for i,d in enumerate(models_info)])\n",
    "    print(f'{dataset_name} Game={game_correct/(len(result_df)):.3f}, {each_model_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afece27-545a-42c1-85a6-5b58e066108f",
   "metadata": {},
   "source": [
    "### MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d52d1-6af0-4100-8e3e-b5497dc0dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMLU \n",
    "def mmlu_init_prompt(q, subject_name):\n",
    "    return f\"You are an expert in {subject_name}. Can you answer the following question as accurately as possible? {q} Explain your answer, putting the answer choice in the form (A) or (B) or (C) or (D) at the end of your response.\"\n",
    "\n",
    "def game_prompt_mmlu(another_resps):\n",
    "    # if len(agents) == 0:\n",
    "    #     return {\"role\": \"user\", \"content\": \"Can you double check that your answer is correct. Put your final answer in the form (X) at the end of your response.\"}\n",
    "    p = \"These are the solutions to the problem from other agents: \"\n",
    "    for r in another_resps:\n",
    "        p += \"\\n\\nOne agent solution: ```{}```\".format(r)\n",
    "    p += \"\"\"\\n\\n Using the reasoning from other agents as additional advice, can you give an updated answer? Examine your solution and that other agents step by step. Put your answer choice in the form (A) or (B) or (C) or (D) at the end of your response.\"\"\"\n",
    "    return p\n",
    "\n",
    "def parse_choice_ans(input_str):\n",
    "    pattern = r'\\(\\s{0,}(\\w)\\s{0,}\\)'\n",
    "    matches = re.findall(pattern, input_str)\n",
    "    solution = None\n",
    "    for match_str in matches[::-1]:\n",
    "        solution = match_str.upper()\n",
    "        if solution: break\n",
    "    return solution\n",
    "\n",
    "def mmlu_df():\n",
    "    task_paths = glob(\"Corpus/MMLU/test/*.csv\")\n",
    "    subjects = list(map(lambda x: x.split('/')[-1].split('_test.csv')[0], task_paths))\n",
    "    mmlu_data_list = []\n",
    "    for task_path, subject in zip(task_paths, subjects):\n",
    "        df = pd.read_csv(task_path, header=None)\n",
    "        for ix in range(len(df)):\n",
    "            question = df.iloc[ix, 0]\n",
    "            a = df.iloc[ix, 1]\n",
    "            b = df.iloc[ix, 2]\n",
    "            c = df.iloc[ix, 3]\n",
    "            d = df.iloc[ix, 4]\n",
    "            # subject, question, answer\n",
    "            mmlu_data_list.append([subject.replace(\"_\", \" \"), f\"{question}: (A) {a}, (B) {b}, (C) {c}, (D) {d}\", df.iloc[ix, 5]])\n",
    "    mmlu_df = pd.DataFrame(mmlu_data_list, columns=['subject', 'question', 'answer'])\n",
    "    return mmlu_df\n",
    "\n",
    "def mmlu_equal(a, b): \n",
    "    if isinstance(a, str) and isinstance(b, str):\n",
    "        return a.upper() == b.upper()\n",
    "    else: return a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab64e3-e4fe-47e4-9d5f-f1163c74d031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ChatLLM import chat_baichuan, chat_qwen2, chat_llama, chat_mistral\n",
    "if __name__ == \"__main__\":\n",
    "    if multiprocessing.get_start_method() != 'spawn':\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "    # model\n",
    "    models_info = [{\"name\": qw_name, \"obj\": qw_model, \"device\": qw_device, \"tokenizer\": qw_tokenizer, \"chatcaller\": chat_qwen2},\n",
    "                  {\"name\": llama_name, \"obj\": llama_model, \"device\": llama_device, \"tokenizer\": llama_tokenizer, \"chatcaller\": chat_llama},\n",
    "                  # {\"name\": baichuan_name, \"obj\": baichuan_model, \"device\": baichuan_device, \"tokenizer\": baichuan_tokenizer, \"chatcaller\": chat_baichuan},\n",
    "                  {\"name\": mistral_name, \"obj\": mistral_model, \"device\": mistral_device, \"tokenizer\": mistral_tokenizer, \"chatcaller\": chat_mistral}]\n",
    "    # hyper params\n",
    "    max_round = 5\n",
    "    timestamp = 20240808\n",
    "    result_dir = \"Results/GameLLM\"\n",
    "    save_last_n_round = 1\n",
    "    if not os.path.exists(result_dir): os.mkdir(result_dir)\n",
    "    # data\n",
    "    dataset_df = mmlu_df()\n",
    "    dataset_name = \"MMLU\"\n",
    "    print(\"MMLU data merged.\")\n",
    "    # main\n",
    "    while True:\n",
    "        try:\n",
    "            brainstorm(\n",
    "                models_info, max_round, save_last_n_round, timestamp, result_dir,# runtime\n",
    "                dataset_df=dataset_df, dataset_name=dataset_name, # data\n",
    "                equal_func=mmlu_equal, init_prompt_template=mmlu_init_prompt, game_prompt_template=game_prompt_mmlu, \n",
    "                parse_ans=parse_choice_ans, # prompt & utils\n",
    "            )\n",
    "            break\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print('OOM')\n",
    "        # except ValueError:\n",
    "        #     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a126e-d5da-43ca-a34c-85839cebda2b",
   "metadata": {},
   "source": [
    "### GSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1feced-507f-4fb7-ba57-a5451dc7f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMLU \n",
    "def gsm_init_prompt(q, subject_name):\n",
    "    question = f\"You are an expert in math. Please solve the following math problem step by step. {q}\\nExplain your reasoning. \"\n",
    "    question += \"Your final answer should be a single numerical number in the form of {answer} at the end of your response.\"\n",
    "    question += \"Make sure that the last numerical number in your response is your final answer.\"\n",
    "    return question\n",
    "\n",
    "def game_prompt_gsm(another_resps):\n",
    "    # if len(agents) == 0:\n",
    "    #     return {\"role\": \"user\", \"content\": \"Can you double check that your answer is correct. Put your final answer in the form (X) at the end of your response.\"}\n",
    "    p = \"These are the solutions to the problem from other agents: \"\n",
    "    for r in another_resps:\n",
    "        p += \"\\n\\nOne agent solution: ```{}```\".format(r)\n",
    "    p += \"\\n\\n Using the solutions from other agents as additional information, can you provide your answer to the math problem? \"\n",
    "    p += \"Examine your solution and other agents' solution step by step. \"\n",
    "    p += \"Your final answer should be a single numerical number in the form of {answer} at the end of your response. \"\n",
    "    p += \"Make sure that the last numerical number in your response is your final answer.\"\n",
    "    return p\n",
    "\n",
    "def parse_gsm_ans(s):\n",
    "    s = s.replace(\",\", \"\").replace(r\"\\$\", \"\")    # case: 70,000 $18\n",
    "    pattern1 = r\"\\{[+-]?\\d+\\.?\\d*\\}\"    # fix minus number\n",
    "    pattern2 = r\"[+-]?\\d+\\.?\\d*\"\n",
    "    if (matches := re.findall(pattern1, s)):\n",
    "        return matches[-1].replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "    elif(matches := re.findall(pattern2, s)):\n",
    "        return matches[-1].replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "    return None\n",
    "\n",
    "import nemo_skills\n",
    "from nemo_skills.code_execution.math_grader import * #math_equal\n",
    "nemo_symbolic_equal = nemo_skills.code_execution.math_grader.symbolic_equal\n",
    "def my_symbolic_equal(_a, _b, tolerance, timeout=10.0):\n",
    "    import sympy\n",
    "    from sympy.parsing.latex import parse_latex\n",
    "    from sympy.parsing.sympy_parser import parse_expr\n",
    "    # expr compare style\n",
    "    try:\n",
    "        a = parse_expr(_a)\n",
    "        b = parse_expr(_b)\n",
    "        try:\n",
    "            with time_limit(timeout*2):\n",
    "                if sympy.simplify(a - b) == 0: return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            with time_limit(timeout):\n",
    "                if isclose(sympy.N(a), sympy.N(b), rel_tol=tolerance): return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    # LaTeX compare style\n",
    "    try:\n",
    "        a = parse_latex(_a)\n",
    "        b = parse_latex(_b)\n",
    "        return a.equals(b)\n",
    "    except Exception:    # I do not understand\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "nemo_skills.code_execution.math_grader.symbolic_equal = my_symbolic_equal\n",
    "# def math_ans_is_equal(resp, label): \n",
    "#     if str(resp).strip() == str(label).strip(): return True\n",
    "#     return math_equal(str(resp), str(label))\n",
    "\n",
    "def gsm_ans_is_equal(resp, label):\n",
    "    # if resp is not None and label is not None:\n",
    "    #     return np.abs(float(label)-float(resp))<1e-5\n",
    "    # return False\n",
    "    if str(resp).strip() == str(label).strip(): return True\n",
    "    return math_equal(str(resp), str(label))\n",
    "\n",
    "def gsm_df():\n",
    "    df = ParquetFile(os.path.expanduser('~/Corpus/gsm8k/main/test-00000-of-00001.parquet')).to_pandas()\n",
    "    gsm_data_list = []\n",
    "    for ix in range(len(df)):\n",
    "            question = df.iloc[ix, 0]\n",
    "            answer = df.iloc[ix, 1].split(\"####\")[-1].strip()\n",
    "            # subject, question, answer\n",
    "            gsm_data_list.append([\"math\", question, answer])\n",
    "    gsm_df = pd.DataFrame(gsm_data_list, columns=['subject', 'question', 'answer'])\n",
    "    return gsm_df\n",
    "\n",
    "#####################################################\n",
    "\n",
    "from ChatLLM import chat_baichuan, chat_qwen2, chat_llama, chat_mistral\n",
    "if __name__ == \"__main__\":\n",
    "    if multiprocessing.get_start_method() != 'spawn':\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "    # model\n",
    "    models_info = [{\"name\": qw_name, \"obj\": qw_model, \"device\": qw_device, \"tokenizer\": qw_tokenizer, \"chatcaller\": chat_qwen2},\n",
    "                  {\"name\": llama_name, \"obj\": llama_model, \"device\": llama_device, \"tokenizer\": llama_tokenizer, \"chatcaller\": chat_llama},\n",
    "                  # {\"name\": baichuan_name, \"obj\": baichuan_model, \"device\": baichuan_device, \"tokenizer\": baichuan_tokenizer, \"chatcaller\": chat_baichuan},\n",
    "                  {\"name\": mistral_name, \"obj\": mistral_model, \"device\": mistral_device, \"tokenizer\": mistral_tokenizer, \"chatcaller\": chat_mistral}]\n",
    "    # hyper params\n",
    "    max_round = 8\n",
    "    timestamp = 20240809\n",
    "    result_dir = \"Results/GameLLM\"\n",
    "    save_last_n_round = 1\n",
    "    if not os.path.exists(result_dir): os.mkdir(result_dir)\n",
    "    # data\n",
    "    dataset_df = gsm_df()\n",
    "    dataset_name = \"GSM\"\n",
    "    print(\"GSM data ready.\")\n",
    "    # main\n",
    "    while True:\n",
    "        try:\n",
    "            brainstorm(\n",
    "                models_info, max_round, save_last_n_round, timestamp, result_dir,# runtime\n",
    "                dataset_df=dataset_df, dataset_name=dataset_name, # data\n",
    "                equal_func=gsm_ans_is_equal, init_prompt_template=gsm_init_prompt, game_prompt_template=game_prompt_gsm, \n",
    "                parse_ans=parse_gsm_ans, # prompt & utils\n",
    "            )\n",
    "            break\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print('OOM')\n",
    "        # except ValueError:\n",
    "        #     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea507d16-4424-47aa-9c98-d37c6999017a",
   "metadata": {},
   "source": [
    "### Arc-e/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf156c7-21ca-4b89-a22e-93257ec7b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMLU \n",
    "def arc_init_prompt(question, subject):\n",
    "    return question\n",
    "\n",
    "def _arc_init_prompt(question, answer, choice_text, choice_label):    \n",
    "    if answer.isdigit():\n",
    "        answer = chr(ord(\"A\")-1+int(answer)) # map digit to A B C D E\n",
    "    question = f\"Can you answer the following question as accurately as possible? {question}: \"\n",
    "    for i,choice in enumerate(choice_text):\n",
    "        if i>0: question += \" \"\n",
    "        question += f\"({chr(ord('A')+i)}) {choice}\"\n",
    "    question += \" Explain your answer, putting the best one answer choice in the form \"\n",
    "    answer_form = \"\"\n",
    "    for i in range(len(choice_label)):\n",
    "        if i>0: answer_form += \"or \"\n",
    "        answer_form += f\"({chr(ord('A')+i)}) \"\n",
    "    question += answer_form + \"at the end of your response.\"\n",
    "    # print(question, answer)\n",
    "    # raise RuntimeError\n",
    "    return question, answer\n",
    "\n",
    "def game_prompt_arc(another_resps):\n",
    "    p = \"These are the solutions to the problem from other agents: \"\n",
    "    for r in another_resps:\n",
    "        p += \"\\n\\nOne agent solution: ```{}```\".format(r)\n",
    "    p += \"\\n\\n Using the reasoning from other agents as additional advice, can you give an updated answer? Examine your solution and that other agents step by step. \"\n",
    "    p += f\"Put your answer choice in the form (A) or (B) or (C) or (D) or (E) at the end of your response.\"\n",
    "    p += f\"Make sure that the last term in the form (A) or (B) or (C) or (D) or (E) in your response is your final answer.\"\n",
    "    return p\n",
    "\n",
    "def arc_ans_is_equal(resp, label): return resp == label\n",
    "\n",
    "def arc_df(difficulty):\n",
    "    df = ParquetFile(os.path.expanduser(f'~/Corpus/ai2_arc/ARC-{difficulty}/test-00000-of-00001.parquet')).to_pandas()\n",
    "    arc_data_list = []\n",
    "    for ix in range(len(df)):\n",
    "        question = df.iloc[ix, 1]\n",
    "        answer = df.iloc[ix, 2]\n",
    "        question, answer = _arc_init_prompt(question, answer, df.iloc[ix, 3], df.iloc[ix, 4])\n",
    "        arc_data_list.append([f\"ARC-{difficulty}\", question, answer])\n",
    "    arc_df = pd.DataFrame(arc_data_list, columns=['subject', 'question', 'answer'])\n",
    "    # print(arc_df.iloc[0,1])\n",
    "    return arc_df\n",
    "\n",
    "#####################################################\n",
    "\n",
    "from ChatLLM import chat_baichuan, chat_qwen2, chat_llama, chat_mistral\n",
    "if __name__ == \"__main__\":\n",
    "    if multiprocessing.get_start_method() != 'spawn':\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "    # model\n",
    "    models_info = [{\"name\": qw_name, \"obj\": qw_model, \"device\": qw_device, \"tokenizer\": qw_tokenizer, \"chatcaller\": chat_qwen2},\n",
    "                  {\"name\": llama_name, \"obj\": llama_model, \"device\": llama_device, \"tokenizer\": llama_tokenizer, \"chatcaller\": chat_llama},\n",
    "                  # {\"name\": baichuan_name, \"obj\": baichuan_model, \"device\": baichuan_device, \"tokenizer\": baichuan_tokenizer, \"chatcaller\": chat_baichuan},\n",
    "                  {\"name\": mistral_name, \"obj\": mistral_model, \"device\": mistral_device, \"tokenizer\": mistral_tokenizer, \"chatcaller\": chat_mistral}]\n",
    "    # hyper params\n",
    "    max_round = 8\n",
    "    timestamp = 20240810\n",
    "    result_dir = \"Results/GameLLM\"\n",
    "    save_last_n_round = 1\n",
    "    if not os.path.exists(result_dir): os.mkdir(result_dir)\n",
    "    # data\n",
    "    # dataset_df = arc_df(\"Easy\")\n",
    "    # dataset_name = \"ARC-e\"\n",
    "    # print(f\"{dataset_name} data ready.\")\n",
    "    # # main\n",
    "    # while True:\n",
    "    #     try:\n",
    "    #         brainstorm(\n",
    "    #             models_info, max_round, save_last_n_round, timestamp, result_dir,# runtime\n",
    "    #             dataset_df=dataset_df, dataset_name=dataset_name, # data\n",
    "    #             equal_func=arc_ans_is_equal, init_prompt_template=arc_init_prompt, game_prompt_template=game_prompt_arc, \n",
    "    #             parse_ans=parse_choice_ans, # prompt & utils\n",
    "    #         )\n",
    "    #         break\n",
    "    #     except torch.cuda.OutOfMemoryError:\n",
    "    #         print('OOM')\n",
    "    # ############################################\n",
    "    dataset_df = arc_df(\"Challenge\")\n",
    "    dataset_name = \"ARC-c\"\n",
    "    print(f\"{dataset_name} data ready.\")\n",
    "    # main\n",
    "    while True:\n",
    "        try:\n",
    "            brainstorm(\n",
    "                models_info, max_round, save_last_n_round, timestamp, result_dir,# runtime\n",
    "                dataset_df=dataset_df, dataset_name=dataset_name, # data\n",
    "                equal_func=arc_ans_is_equal, init_prompt_template=arc_init_prompt, game_prompt_template=game_prompt_arc, \n",
    "                parse_ans=parse_choice_ans, # prompt & utils\n",
    "            )\n",
    "            break\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print('OOM')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229db2c-fbdd-44c5-b382-e29c495064ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty = 'Easy'\n",
    "df = ParquetFile(os.path.expanduser(f'~/Corpus/ai2_arc/ARC-{difficulty}/test-00000-of-00001.parquet')).to_pandas()\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d07d1c-c32a-40b9-8602-59a221758000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de905b-ddaf-459c-b2d2-c4249e67ec99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc5f63-0819-4e0c-86c4-7b50f8099062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9dec56-80be-4f90-a4ef-4ae754cabea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7055a-f8c2-400a-a60b-23e98166acc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd0bfa-f699-492e-8534-01dbfa90bbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef98aa-7c3f-461f-9e97-602538a695fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40cea52c-57ba-4180-80c9-3099a77e286a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db83fcb-6e27-45d5-b1e4-53f41322ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"in the form (X) <-> in between the brakets \"\"\"\n",
    "\"\"\"put your answer ++choice++\n",
    "A) -> (A)\n",
    "in the form (A) or (B) or (C) or (D)\n",
    "\"\"\"\n",
    "def parse_mmlu(df, ix, subject_name):\n",
    "    question = df.iloc[ix, 0]\n",
    "    a = df.iloc[ix, 1]\n",
    "    b = df.iloc[ix, 2]\n",
    "    c = df.iloc[ix, 3]\n",
    "    d = df.iloc[ix, 4]\n",
    "    question = \"You are an expert in {}. Can you answer the following question as accurately as possible? {}: (A) {}, (B) {}, (C) {}, (D) {} Explain your answer, putting the answer choice in the form (A) or (B) or (C) or (D) at the end of your response.\".format(subject_name, question, a, b, c, d)\n",
    "    answer = df.iloc[ix, 5]\n",
    "    return question, answer\n",
    "\n",
    "def game_prompt_mmlu(another_resp):\n",
    "    # if len(agents) == 0:\n",
    "    #     return {\"role\": \"user\", \"content\": \"Can you double check that your answer is correct. Put your final answer in the form (X) at the end of your response.\"}\n",
    "    prefix_string = \"These are the solutions to the problem from other agents: \\n\\n One agent solution: ```{}```\".format(another_resp)\n",
    "    prefix_string += \"\"\"\\n\\n Using the reasoning from other agents as additional advice, can you give an updated answer? Examine your solution and that other agents step by step. Put your answer choice in the form (A) or (B) or (C) or (D) at the end of your response.\"\"\"\n",
    "    return prefix_string\n",
    "\n",
    "def parse_choice_ans(input_str):\n",
    "    pattern = r'\\(\\s{0,}(\\w)\\s{0,}\\)'\n",
    "    matches = re.findall(pattern, input_str)\n",
    "    solution = None\n",
    "    for match_str in matches[::-1]:\n",
    "        solution = match_str.upper()\n",
    "        if solution: break\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22555f56-2851-4adc-95d9-aeac283e1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_run(model_a, model_b, tkz_a, tkz_b, device_a, device_b, chat_model_a, chat_model_b):\n",
    "    # task paths\n",
    "    task_paths = glob(\"Corpus/MMLU/test/*.csv\")\n",
    "    subjects = list(map(lambda x: x.split('/')[-1].split('_test.csv')[0], task_paths))\n",
    "    # build result df\n",
    "    result_csv_filename = f\"MMLU_{model_a_name}_{model_b_name}_{timestamp}_{max_round}_Last{save_last_n_round}Round.csv\"\n",
    "    csv_path = os.path.join(result_dir, result_csv_filename)\n",
    "    if os.path.exists(csv_path):\n",
    "        result_df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        result_df = pd.DataFrame(columns=['subject','ith','label','a_first_ans','b_first_ans','a_game_ans','b_game_ans','rounds'])\n",
    "        result_df.to_csv(csv_path, index=False)\n",
    "    def write_to_result_df(result_df, subject, i, answer, a_first_ans, b_first_ans, a_game_answer, b_game_answer, g):\n",
    "        result_df = pd.concat([result_df, pd.DataFrame([{\n",
    "                'subject': subject, 'ith':i, 'label':answer,\n",
    "                'a_first_ans':a_first_ans,'b_first_ans':b_first_ans,\n",
    "                'a_game_ans':a_game_answer,'b_game_ans':b_game_answer,'rounds':g\n",
    "            }])],ignore_index=True)\n",
    "        result_df.to_csv(os.path.join(result_dir, result_csv_filename), index=False)\n",
    "        return result_df\n",
    "    \n",
    "    for task_path, subject in zip(task_paths, subjects):\n",
    "        # fetch statistic\n",
    "        df = pd.read_csv(task_path, header=None)\n",
    "        if (result_lines:=len(result_df[result_df['subject']==subject]))>0:\n",
    "            if result_lines==len(df): continue\n",
    "            label_so_far = result_df[result_df['subject']==subject].label.to_list()\n",
    "            model_a_so_far = result_df[result_df['subject']==subject].a_first_ans.to_list()\n",
    "            model_b_so_far = result_df[result_df['subject']==subject].b_first_ans.to_list()\n",
    "            game_a_so_far = result_df[result_df['subject']==subject].a_game_ans.to_list()\n",
    "            game_b_so_far = result_df[result_df['subject']==subject].b_game_ans.to_list()\n",
    "            a_correct = list(map(lambda x: x[0]==x[1], zip(label_so_far, model_a_so_far))).count(True)\n",
    "            b_correct = list(map(lambda x: x[0]==x[1], zip(label_so_far, model_b_so_far))).count(True)\n",
    "            game_correct = list(map(lambda x: x[0]==x[1]==x[2], zip(label_so_far, game_a_so_far, game_b_so_far))).count(True)\n",
    "            # subject_start_idx = len(label_so_far)\n",
    "            # if len(df) == subject_start_idx:\n",
    "            #     continue\n",
    "        else:\n",
    "            a_correct, b_correct, game_correct, subject_start_idx = 0,0,0,0\n",
    "        # print(f\"Current subjuect: {subject}\")\n",
    "        # into one subject\n",
    "        subj_slice = result_df[result_df['subject']==subject]\n",
    "        for idx in (pbar:=tqdm(range(len(df)))):\n",
    "            if len(subj_slice)>0:    # skip processed line by line\n",
    "                if idx in subj_slice['ith'].values: continue\n",
    "            # save dialog\n",
    "            dialog_dir = os.path.join(result_dir, f\"dialog_{timestamp}_LastOnly\")\n",
    "            if not os.path.exists(dialog_dir): os.mkdir(dialog_dir)\n",
    "            dialog_filename = f\"{model_a_name}_{model_b_name}_{timestamp}_{subject}_{idx}_LastOnly.txt\"\n",
    "            dialog_fp = open(os.path.join(dialog_dir, dialog_filename), 'w+', encoding=\"utf-8\")\n",
    "            # game rounds\n",
    "            answers_for_majority = []\n",
    "            question, answer = parse_mmlu(df, idx, subject.replace(\"_\", \" \"))\n",
    "            for round in range(max_round):\n",
    "                if round == 0:\n",
    "                    pool = Pool(processes=2)\n",
    "                    result_a = pool.apply_async(chat_model_a, (model_a, tkz_a, device_a, question,))\n",
    "                    result_b = pool.apply_async(chat_model_b, (model_b, tkz_b, device_b, question,))\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "                    a_resp, a_history = result_a.get()\n",
    "                    b_resp, b_msgs = result_b.get()\n",
    "                    # a_resp, a_history = chat_model_a(model_a, tkz_a, device_a, question)\n",
    "                    # b_resp, b_msgs = chat_model_b(model_b, tkz_b, device_b, question)\n",
    "                else:    # ans_a != ans_b\n",
    "                    if round > save_last_n_round:    # shave history\n",
    "                        a_history = a_history[:(3 if a_history[0]['role'] == \"system\" else 2)] + a_history[-2*save_last_n_round:]\n",
    "                        b_msgs = b_msgs[:(3 if b_msgs[0]['role'] == \"system\" else 2)] + b_msgs[-2*save_last_n_round:]\n",
    "                    game_a_prompt, game_b_prompt = game_prompt_mmlu(b_resp), game_prompt_mmlu(a_resp)\n",
    "                    # a_resp, a_history = chat_model_a(model_a, tkz_a, device_a, game_a_prompt, a_history)\n",
    "                    # b_resp, b_msgs = chat_model_b(model_b, tkz_b, device_b, game_b_prompt, b_msgs)\n",
    "                    pool = Pool(processes=2)\n",
    "                    result_a = pool.apply_async(chat_model_a, (model_a, tkz_a, device_a, game_a_prompt, a_history,))\n",
    "                    result_b = pool.apply_async(chat_model_b, (model_b, tkz_b, device_b, game_b_prompt, b_msgs,))\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "                    a_resp, a_history = result_a.get()\n",
    "                    b_resp, b_msgs = result_b.get()\n",
    "                # save response\n",
    "                dialog_fp.write(f\"a: {a_resp}\\nb: {b_resp}\\n\\n\")\n",
    "                ans_a, ans_b = parse_choice_ans(a_resp), parse_choice_ans(b_resp)\n",
    "                # save answer for voting\n",
    "                if 'MMLU' in task_path.upper():\n",
    "                    if ans_a in ['A', 'B', 'C', 'D']: answers_for_majority.append(ans_a)\n",
    "                    if ans_b in ['A', 'B', 'C', 'D']: answers_for_majority.append(ans_b)\n",
    "                # control game flow\n",
    "                if round == 0: \n",
    "                    a_first_ans, b_first_ans = ans_a, ans_b\n",
    "                    if ans_a == answer: a_correct += 1\n",
    "                    if ans_b == answer: b_correct += 1\n",
    "                if ans_a == ans_b:\n",
    "                    if ans_a == answer:\n",
    "                        game_correct += 1\n",
    "                    result_df = write_to_result_df(result_df, subject, idx, answer, a_first_ans, b_first_ans, ans_a, ans_b, round)\n",
    "                    break\n",
    "                # majority vote when no converge\n",
    "                if round == max_round -1:\n",
    "                    try:\n",
    "                        v = Counter(answers_for_majority)\n",
    "                        vote_result = v.most_common(1)[0][0]\n",
    "                    except:\n",
    "                        vote_result = np.random.choice([\"A\",\"B\",\"C\",\"D\"])\n",
    "                    result_df = write_to_result_df(result_df, subject, idx, answer, a_first_ans, b_first_ans, vote_result, vote_result, round)\n",
    "                    answers_for_majority.clear()\n",
    "                    break\n",
    "            # Destructor\n",
    "            del a_history, b_msgs\n",
    "            dialog_fp.close()\n",
    "            pbar.set_description(f\"Subject: {subject} Acc: Game={game_correct/(idx+1):.3f}, {model_a_name}={a_correct/(idx+1):.3f}, {model_b_name}={b_correct/(idx+1):.3f}\")\n",
    "        print(f\"Subject: {subject} Acc: Game={game_correct/(idx+1):.3f}, {model_a_name}={a_correct/(idx+1):.3f}, {model_b_name}={b_correct/(idx+1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97e29b-3bba-4e11-8d75-0a232b0af41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ChatLLM import chat_baichuan, chat_qwen, chat_phi, chat_llama, chat_mistral\n",
    "if __name__ == \"__main__\":\n",
    "    if multiprocessing.get_start_method() != 'spawn':\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "    while True:\n",
    "        try:\n",
    "            main_run(\n",
    "                model_a=baichuan_model, \n",
    "                model_b=mistral_model, \n",
    "                tkz_a=baichuan_tokenizer, \n",
    "                tkz_b=mistral_tokenizer, \n",
    "                device_a=baichuan_device, \n",
    "                device_b=mistral_device, \n",
    "                chat_model_a=chat_baichuan, \n",
    "                chat_model_b=chat_mistral\n",
    "            )\n",
    "            break\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            pass\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e1ecc-273a-46a3-8df3-37306ef762c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0cff192-1800-4ebc-938b-7149c74a884a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GSM8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870405d6-5355-46ea-871e-3345673ab1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_last_n_round = 1\n",
    "\n",
    "from fastparquet import ParquetFile\n",
    "def parse_gsm(df, ix):\n",
    "    question = df.iloc[ix, 0]\n",
    "    answer = df.iloc[ix, 1].split(\"####\")[-1].strip()\n",
    "    question = f\"You are an expert in math. Please solve the following math problem step by step. {question}\\nExplain your reasoning. \"\n",
    "    question += \"Your final answer should be a single numerical number in the form of {answer} at the end of your response.\"\n",
    "    question += \"Make sure that the last numerical number in your response is your final answer.\"\n",
    "    return question, answer\n",
    "\n",
    "def game_prompt_gsm(another_resp):\n",
    "    prefix_string = \"These are the solutions to the problem from other agents: \\n\\n One agent solution: ```{}```\".format(another_resp)\n",
    "    prefix_string += \"\\n\\n Using the solutions from other agents as additional information, can you provide your answer to the math problem? \"\n",
    "    prefix_string += \"Examine your solution and other agents' solution step by step. \"\n",
    "    prefix_string += \"Your final answer should be a single numerical number in the form of {answer} at the end of your response. \"\n",
    "    prefix_string += \"Make sure that the last numerical number in your response is your final answer.\"\n",
    "    return prefix_string\n",
    "\n",
    "def parse_gsm_ans(s):\n",
    "    s = s.replace(\",\", \"\").replace(r\"\\$\", \"\")    # case: 70,000 $18\n",
    "    pattern1 = r\"\\{[+-]?\\d+\\.?\\d*\\}\"    # fix minus number\n",
    "    pattern2 = r\"[+-]?\\d+\\.?\\d*\"\n",
    "    if (matches := re.findall(pattern1, s)):\n",
    "        return matches[-1].replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "    elif(matches := re.findall(pattern2, s)):\n",
    "        return matches[-1].replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "    return None\n",
    "\n",
    "def gsm_ans_is_equal(resp, label):\n",
    "    if resp is not None and label is not None:\n",
    "        return np.abs(float(label)-float(resp))<1e-5\n",
    "    return False\n",
    "\n",
    "# for ix in range(len(gsm_test_df)):\n",
    "#     q, a = parse_gsm(gsm_test_df,ix)\n",
    "#     if a[0] == '-': print(a)\n",
    "#     # if \"####\" in gsm_test_df.iloc[ix, 1]: print(ix)\n",
    "\n",
    "def main_gsm(model_a, model_b, tkz_a, tkz_b, device_a, device_b, chat_model_a, chat_model_b):\n",
    "    gsm_test_df = ParquetFile(os.path.expanduser('~/Corpus/gsm8k/main/test-00000-of-00001.parquet')).to_pandas()\n",
    "    # build result df\n",
    "    result_csv_filename = f\"GSM_{model_a_name}_{model_b_name}_{timestamp}_{max_round}_Last{save_last_n_round}Round.csv\"\n",
    "    csv_path = os.path.join(result_dir, result_csv_filename)\n",
    "    if os.path.exists(csv_path):\n",
    "        result_df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        result_df = pd.DataFrame(columns=['ith','label','a_first_ans','b_first_ans','a_game_ans','b_game_ans','rounds'])\n",
    "        result_df.to_csv(csv_path, index=False)\n",
    "    def gsm_to_result_df(result_df, i, answer, a_first_ans, b_first_ans, a_game_answer, b_game_answer, g):\n",
    "        result_df = pd.concat([result_df, pd.DataFrame([{\n",
    "                'ith':i, 'label':answer,\n",
    "                'a_first_ans':a_first_ans,'b_first_ans':b_first_ans,\n",
    "                'a_game_ans':a_game_answer,'b_game_ans':b_game_answer,'rounds':g\n",
    "            }])],ignore_index=True)\n",
    "        result_df.to_csv(os.path.join(result_dir, result_csv_filename), index=False)\n",
    "        return result_df\n",
    "\n",
    "    restart = True\n",
    "    for idx in (pbar:=tqdm(range(len(gsm_test_df)))):\n",
    "        if len(result_df)>0:    # skip processed line by line\n",
    "            if idx in result_df['ith'].values: continue\n",
    "            if restart:\n",
    "                label_so_far = result_df.label.to_list()\n",
    "                model_a_so_far = result_df.a_first_ans.to_list()\n",
    "                model_b_so_far = result_df.b_first_ans.to_list()\n",
    "                game_a_so_far = result_df.a_game_ans.to_list()\n",
    "                game_b_so_far = result_df.b_game_ans.to_list()\n",
    "                a_correct = list(map(lambda x: gsm_ans_is_equal(x[0],x[1]), zip(label_so_far, model_a_so_far))).count(True)\n",
    "                b_correct = list(map(lambda x: gsm_ans_is_equal(x[0],x[1]), zip(label_so_far, model_b_so_far))).count(True)\n",
    "                game_correct = list(map(lambda x: gsm_ans_is_equal(x[0],x[1]) and gsm_ans_is_equal(x[2],x[1]), \n",
    "                                        zip(label_so_far, game_a_so_far, game_b_so_far))).count(True)\n",
    "                restart = False\n",
    "        else:\n",
    "            a_correct, b_correct, game_correct, subject_start_idx = 0,0,0,0\n",
    "        # save dialog\n",
    "        dialog_dir = os.path.join(result_dir, f\"dialog_{timestamp}_LastOnly\")\n",
    "        if not os.path.exists(dialog_dir): os.mkdir(dialog_dir)\n",
    "        dialog_filename = f\"GSM_{model_a_name}_{model_b_name}_{timestamp}_{idx}_LastOnly.txt\"\n",
    "        dialog_fp = open(os.path.join(dialog_dir, dialog_filename), 'w+', encoding=\"utf-8\")\n",
    "        # game rounds\n",
    "        answers_for_majority = []\n",
    "        question, answer = parse_gsm(gsm_test_df,idx)\n",
    "        for round in range(max_round):\n",
    "            if round == 0:\n",
    "                pool = Pool(processes=2)\n",
    "                result_a = pool.apply_async(chat_model_a, (model_a, tkz_a, device_a, question,))\n",
    "                result_b = pool.apply_async(chat_model_b, (model_b, tkz_b, device_b, question,))\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "                a_resp, a_history = result_a.get()\n",
    "                b_resp, b_msgs = result_b.get()\n",
    "            else:    # ans_a != ans_b\n",
    "                if round > save_last_n_round:    # shave history\n",
    "                    # print(b_msgs)\n",
    "                    # return\n",
    "                    a_history = a_history[:(3 if a_history[0]['role'] == \"system\" else 2)] + a_history[-2*save_last_n_round:]\n",
    "                    b_msgs = b_msgs[:(3 if b_msgs[0]['role'] == \"system\" else 2)] + b_msgs[-2*save_last_n_round:]\n",
    "                game_a_prompt, game_b_prompt = game_prompt_gsm(b_resp), game_prompt_gsm(a_resp)\n",
    "                pool = Pool(processes=2)\n",
    "                result_a = pool.apply_async(chat_model_a, (model_a, tkz_a, device_a, game_a_prompt, a_history))\n",
    "                result_b = pool.apply_async(chat_model_b, (model_b, tkz_b, device_b, game_b_prompt, b_msgs))\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "                a_resp, a_history = result_a.get()\n",
    "                b_resp, b_msgs = result_b.get()\n",
    "            # save response\n",
    "            dialog_fp.write(f\"a: {a_resp}\\nb: {b_resp}\\n\\n\")\n",
    "            ans_a, ans_b = parse_gsm_ans(a_resp), parse_gsm_ans(b_resp)\n",
    "            # save answer for voting\n",
    "            answers_for_majority.append(ans_a)\n",
    "            answers_for_majority.append(ans_b)\n",
    "            # control game flow\n",
    "            if round == 0: \n",
    "                a_first_ans, b_first_ans = ans_a, ans_b\n",
    "                if gsm_ans_is_equal(ans_a, answer.replace(',',\"\")): a_correct += 1\n",
    "                if gsm_ans_is_equal(ans_b, answer.replace(',',\"\")): b_correct += 1\n",
    "            if gsm_ans_is_equal(ans_a, ans_b):\n",
    "                if gsm_ans_is_equal(ans_a, answer.replace(',',\"\")): game_correct += 1\n",
    "                result_df = gsm_to_result_df(result_df, idx, answer, a_first_ans, b_first_ans, ans_a, ans_b, round)\n",
    "                break\n",
    "            # majority vote when no converge\n",
    "            if round == max_round -1:\n",
    "                try:\n",
    "                    v = Counter(answers_for_majority)\n",
    "                    vote_result = v.most_common(1)[0][0]\n",
    "                except:\n",
    "                    vote_result = None\n",
    "                result_df = gsm_to_result_df(result_df, idx, answer, a_first_ans, b_first_ans, vote_result, vote_result, round)\n",
    "                answers_for_majority.clear()\n",
    "                # Destructor\n",
    "                del a_history, b_msgs\n",
    "                dialog_fp.close()\n",
    "                break\n",
    "        pbar.set_description(f\"GSM Acc: Game={game_correct/(idx+1):.3f}, {model_a_name}={a_correct/(idx+1):.3f}, {model_b_name}={b_correct/(idx+1):.3f}\")\n",
    "    print(f\"GSM Acc: Game={game_correct/(idx+1):.3f}, {model_a_name}={a_correct/(idx+1):.3f}, {model_b_name}={b_correct/(idx+1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79b134-0c2e-44cb-a515-a065c6fca15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ChatLLM import chat_baichuan, chat_qwen, chat_phi, chat_llama, chat_mistral\n",
    "if __name__ == \"__main__\":\n",
    "    if multiprocessing.get_start_method() != 'spawn':\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "    while True:\n",
    "        try:\n",
    "            main_gsm(\n",
    "                model_a=baichuan_model, \n",
    "                model_b=mistral_model, \n",
    "                tkz_a=baichuan_tokenizer, \n",
    "                tkz_b=mistral_tokenizer, \n",
    "                device_a=baichuan_device, \n",
    "                device_b=mistral_device, \n",
    "                chat_model_a=chat_baichuan, \n",
    "                chat_model_b=chat_mistral\n",
    "            )\n",
    "            break\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            pass\n",
    "        # except ValueError:\n",
    "        #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da22fffd-1799-44c0-84e7-d85c1068dd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
